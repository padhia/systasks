{% from 'util.jinja2' import date_range %}

{% macro seconds(name) -%}
ZEROIFNULL(CAST(EXTRACT(HOUR FROM {{ name }}) * 3600 + EXTRACT(MINUTE FROM {{ name }}) * 60 + EXTRACT(SECOND FROM {{ name }}) AS FLOAT))
{%- endmacro %}

version: &version "1.0"
description: >
  DBQL Core data

tasks:
{% include "dim_app.jinja2" %}

{% include "dim_user.jinja2" %}

{% include "dim_statement.jinja2" %}

- name: save UTC offset
  connect: source
  export:
    sql: SELECT SUBSTRING ((Current_Time (FORMAT 'HH:MI:SS.S(F)Z') (VARCHAR (20))) FROM 9 FOR 6) as UTC_Offset
    file: UTC_Offset.csv

- name: export MaxCPU
  connect: source
  export:
    file: cpu_summary.csv
    sql: |
      Select '{{ siteid }}' as Site_ID
        , cast(TheDate as format 'Y4-MM-DD') as LogDate
        , Floor(TheTime/1e4) as LogHour
        , SUBSTRING ((Current_Time (FORMAT 'HH:MI:SS.S(F)Z') (VARCHAR (20))) FROM 9 FOR 6) as UTC_Offset
        , cast(max(NodeType) as varchar(10)) as Node_Type
        , cast(count(distinct NodeID) as smallint) as Node_Cnt
        , cast(max(NCPUs) as smallint) as vCPU_per_Node
        , sum(cast(FullPotentialIOTA/1e9 as decimal(18,0))) as MaxIOTA_cntB
        , sum(cast(CPUIdle   as decimal(18,2))) as CPU_Idle
        , sum(cast(CPUIOWait as decimal(18,2))) as CPU_IOWait
        , sum(cast(CPUUServ  as decimal(18,2))) as CPU_OS
        , sum(cast(CPUUExec  as decimal(18,2))) as CPU_DBS
        , CPU_Idle+CPU_IOWait+CPU_OS+CPU_DBS as CPU_Total
      from {{ dbc.spma }}
      where TheDate {{ date_range() }}
      Group by LogDate, LogHour

- name: set timezone to UTC
  connect: source
  execute:
    sql: SET TIME ZONE 'GMT'

- name: export DBQL_Core.csv
  # DBQL_CORE ()
  # =========================
  # Aggregates DBQL into a per-day, per-hour, per-DIM (app/statement/user) buckets
  # as defined above.  The intention is this to be a smaller table than the
  # detail, however, this assumption largely relies on how well the bucketing
  # logic is defined / setup above.  If the result set is too large, try
  # revisiting the Bucket definitions above, and make groups smaller / less varied.

  # Also - many compound metrics have been stripped, to minimize transfer file
  # size.  As these fields are easily calculated, they will be re-constituted
  # in Transcend.

  # dbql_core pre-aggregate to reduce natural skew causing spool-outs

  connect: source
  export:
    file: DBQL_Core.csv
    sql: |
      SELECT Site_ID
          , LogTS
          , max(Total_AMPs) as Total_AMPs
          , app.App_Bucket
          , app.Use_Bucket
          , stm.Statement_Bucket
          , usr.User_Bucket
          , usr.User_Department
          , usr.User_SubDepartment
          , sum(Request_Cnt                ) as Request_Cnt
          , sum(Query_Cnt                  ) as Query_Cnt
          , sum(Query_MultiStatement_Cnt   ) as Query_MultiStatement_Cnt
          , sum(Query_Error_Cnt            ) as Query_Error_Cnt
          , sum(Query_Abort_Cnt            ) as Query_Abort_Cnt
          , sum(Query_NoIO_cnt             ) as Query_NoIO_cnt
          , sum(Query_InMem_Cnt            ) as Query_InMem_Cnt
          , sum(Query_PhysIO_Cnt           ) as Query_PhysIO_Cnt
          , sum(Query_Tactical_Cnt         ) as Query_Tactical_Cnt
          , avg(Query_Complexity_Score_Avg ) as Query_Complexity_Score_Avg
          , sum(Returned_Row_Cnt           ) as Returned_Row_Cnt
          , sum(DelayTime_Sec              ) as DelayTime_Sec
          , sum(RunTime_Parse_Sec          ) as RunTime_Parse_Sec
          , sum(Runtime_AMP_Sec            ) as Runtime_AMP_Sec
          , sum(RunTime_Total_Sec          ) as RunTime_Total_Sec
          , sum(TransferTime_Sec           ) as TransferTime_Sec
          , sum(CPU_Parse_Sec              ) as CPU_Parse_Sec
          , sum(CPU_AMP_Sec                ) as CPU_AMP_Sec
          , sum(IOCntM_Physical            ) as IOCntM_Physical
          , sum(IOCntM_Total               ) as IOCntM_Total
          , sum(IOGB_Physical              ) as IOGB_Physical
          , sum(IOGB_Total                 ) as IOGB_Total
          , sum(IOTA_Used_cntB             ) as IOTA_Used_cntB
          , avg(NumOfActiveAMPs_Avg        ) as NumOfActiveAMPs_Avg
          , sum(Spool_GB                   ) as Spool_GB
          , sum(CacheHit_Pct               ) as CacheHit_Pct
          , avg(CPUSec_Skew_AvgPCt         ) as CPUSec_Skew_AvgPCt
          , avg(IOCnt_Skew_AvgPct          ) as IOCnt_Skew_AvgPct
      FROM (
        SELECT '{{ siteid }}'  as Site_ID
          , cast(cast(starttime as format 'YYYY-MM-DDBHH') AS CHAR(13)) || ':00:00' as LogTS
          , cast(HashAmp()+1 as Integer) as Total_AMPs
          , username
          , appid
          , StatementType

          /* ====== Query Metrics ======= */
          , zeroifnull(cast(count(1) as BigInt)) as Request_Cnt
          , zeroifnull(sum(cast( dbql.Statements as BigInt))) as Query_Cnt
          , zeroifnull(sum(cast(case when dbql.StatementGroup like '%=%' then 1 else 0 end as SmallInt))) as Query_MultiStatement_Cnt
          /* ErrorCode 3158 == TASM Demotion, aka warning, not real error */
          , zeroifnull(sum(cast(case when dbql.ErrorCode not in(0,3158)      then dbql.Statements else 0 end as int))) as Query_Error_Cnt
          , zeroifnull(sum(cast(case when dbql.Abortflag = 'T'               then dbql.Statements else 0 end as int))) as Query_Abort_Cnt
          , zeroifnull(sum(cast(case when TotalIOCount = 0                   then dbql.Statements else 0 end as int))) as Query_NoIO_cnt
          , zeroifnull(sum(cast(case when TotalIOCount > 0 AND ReqPhysIO = 0 then dbql.Statements else 0 end as int))) as Query_InMem_Cnt
          , zeroifnull(sum(cast(case when TotalIOCount > 0 AND ReqPhysIO > 0 then dbql.Statements else 0 end as int))) as Query_PhysIO_Cnt
          , zeroifnull(sum(cast(
              case
                  when dbql.StatementType = 'Select'
                    and dbql.NumOfActiveAMPs < (Total_AMPs * 0.10)
                    and {{ seconds('((FirstRespTime - FirstStepTime) HOUR(3) TO SECOND(6))') }} <= 1  /* Runtime_AMP_Sec */
                  then 1 else 0 end as Integer))) as Query_Tactical_Cnt
          ,avg(dbql.NumSteps) as Query_Complexity_Score_Avg
          ,zeroifnull(sum(cast(dbql.NumResultRows as BigInt) )) as Returned_Row_Cnt

          /* ====== Metrics: RunTimes ====== */
          ,sum(cast(dbql.DelayTime as decimal(18,2))) as DelayTime_Sec
          ,sum({{ seconds('((FirstStepTime - StartTime)     HOUR(3) TO SECOND(6))') }}) as RunTime_Parse_Sec
          ,sum({{ seconds('((FirstRespTime - FirstStepTime) HOUR(3) TO SECOND(6))') }}) as Runtime_AMP_Sec
          ,sum(TotalFirstRespTime)                                                      as RunTime_Total_Sec
          ,sum({{ seconds('((LastRespTime - FirstRespTime)  HOUR(3) TO SECOND(6))') }}) as TransferTime_Sec

          /* ====== Metrics: CPU & IO ====== */
          ,zeroifnull(sum( cast(dbql.ParserCPUTime    as decimal(18,2)))) as CPU_Parse_Sec
          ,zeroifnull(sum( cast(dbql.AMPCPUtime       as decimal(18,2)))) as CPU_AMP_Sec
          /* TODO: check if failed queries log CPU consumption */
          ,zeroifnull(sum( cast(ReqPhysIO/1e6         as decimal(18,2)))) as IOCntM_Physical
          ,zeroifnull(sum( cast(TotalIOCount/1e6      as decimal(18,2)))) as IOCntM_Total
          ,zeroifnull(sum( cast(ReqPhysIOKB/1e6       as decimal(18,2)))) as IOGB_Physical
          ,zeroifnull(sum( cast(ReqIOKB/1e6           as decimal(18,2)))) as IOGB_Total
          ,zeroifnull(sum( cast(dbql.UsedIOTA/1e9     as decimal(18,2)))) as IOTA_Used_cntB

          /* ====== Metrics: Other ====== */
          ,zeroifnull(avg(NumOfActiveAMPs)) as NumOfActiveAMPs_Avg
          ,zeroifnull(sum(SpoolUsage/1e9))  as Spool_GB
          ,zeroifnull(avg(1-(ReqPhysIO/nullifzero(TotalIOCount)))) as CacheHit_Pct
          ,zeroifnull(avg((AMPCPUTime / nullifzero(MaxAmpCPUTime*NumOfActiveAMPs))-1)) as CPUSec_Skew_AvgPCt
          ,zeroifnull(avg((TotalIOCount / nullifzero(MaxAmpIO*NumOfActiveAMPs))-1) )   as IOCnt_Skew_AvgPct

        From {{ dbc.log }} as dbql
        where {{ dbc.logdt }} {{ date_range() }}
        Group by LogTS, Site_ID, username, appid, StatementType

        union all

        SELECT
          '{{ siteid }}'  as Site_ID
          ,cast(cast(starttime as format 'YYYY-MM-DDBHH') AS CHAR(13)) || ':00:00' as LogTS
          ,HashAmp() + 1 as Total_AMPs
          ,username
          ,appid
          ,'Summary' as StatementType
          ,zeroifnull(sum(cast(smry.QueryCount as decimal(18,2)))) as Request_Cnt
          ,zeroifnull(sum(cast(smry.QueryCount as decimal(18,2)))) as Query_Cnt
          ,null as Query_MultiStatement_Cnt
          ,null as Query_Error_Cnt
          ,null as Query_Abort_Cnt
          ,null as Query_NoIO_cnt
          ,null as Query_InMem_Cnt
          ,null as Query_PhysIO_Cnt
          ,null as Query_Tactical_Cnt
          ,null as Query_Complexity_Score_Avg
          ,null as Returned_Row_Cnt
          ,null as DelayTime_Sec
          ,null as RunTime_Parse_Sec
          ,null as Runtime_AMP_Sec
          ,zeroifnull(sum(cast(smry.QuerySeconds as decimal(18,2)))) as RunTime_Total_Sec
          ,null as TransferTime_Sec
          ,zeroifnull(sum(cast(smry.ParserCPUTime as decimal(18,2)))) as CPU_Parse_Sec
          ,zeroifnull(sum(cast(smry.AMPCPUTime as decimal(18,2)))) as CPU_AMP_Sec
          ,zeroifnull(sum(cast(smry.ReqPhysIO/1e6 as decimal(18,2)))) as IOCntM_Physical
          ,zeroifnull(sum(cast(smry.TotalIOCount/1e6 as decimal(18,2)))) as IOCntM_Total
          ,zeroifnull(sum(cast(smry.ReqPhysIOKB/1e6 as decimal(18,2)))) as IOGB_Physical
          ,null as IOGB_Total
          ,zeroifnull(sum(cast(smry.UsedIota/1e9 as decimal(18,2)))) as IOTA_Used_cntB
          ,null as NumOfActiveAMPs_Avg
          ,null as Spool_GB
          ,zeroifnull(avg(1-(ReqPhysIO/nullifzero(TotalIOCount)))) as CacheHit_Pct
          ,null as CPUSec_Skew_AvgPCt
          ,null as IOCnt_Skew_AvgPct

        From {{ dbc.summary }} smry
        where {{ dbc.logdt }} {{ date_range() }}
        Group by LogTS, Site_ID, username, appid, StatementType

      ) dbql
      join dim_app as app     on dbql.AppID = app.AppID
      join dim_Statement stm  on dbql.StatementType = stm.StatementType
      join dim_user usr       on dbql.UserName = usr.UserName

      Group by Site_ID
        , LogTS
        , app.App_Bucket
        , app.Use_Bucket
        , stm.Statement_Bucket
        , usr.User_Bucket
        , usr.User_Department
        , usr.User_SubDepartment

- name: Query_Breakouts by User Buckets
  connect: source
  export:
    file: Query_Breakouts.csv
    sql: |
      SELECT '{{ siteid }}' as Site_ID
        , cast({{ dbc.logdt }} as format 'Y4-MM-DD') as LogDate
        , usr.User_Bucket
        , usr.User_Department
        , usr.User_SubDepartment
        {% for time_col, time_name in [('TotalFirstResp', 'run'), ('Delay', 'delay')] %}
        {% for calc, title in [('dbql.Statements', 'qrycnt'), ('dbql.AMPCPUtime + dbql.ParserCPUTime', 'cpusec'), ('ReqIOKB/1e6', 'iogb')] %}

        , zeroifnull(SUM(CAST(CASE WHEN dbql.{{ time_col }}Time is NULL  OR dbql.{{ time_col }}Time < 1    THEN {{ calc }} ELSE 0 END AS INTEGER)))   as {{ title }}_in_{{ time_name }}time_0000_0001
        {% for lower_lim, upper_lim in [(1,5), (5,10), (10,30), (30,60), (60,300), (300,600), (600,1800), (1800,3600)] %}
        , zeroifnull(SUM(CAST(CASE WHEN dbql.{{ time_col }}Time >= {{ lower_lim | pyformat('4d') }} AND dbql.{{ time_col }}Time < {{ upper_lim | pyformat('4d') }} THEN {{ calc }} ELSE 0 END AS INTEGER)))   as {{ title }}_in_{{ time_name }}time_{{ lower_lim | pyformat('04d') }}_{{ upper_lim | pyformat('04d') }}
        {% endfor %}
        , zeroifnull(SUM(CAST(CASE WHEN dbql.{{ time_col }}Time >  3600  THEN {{ calc }} ELSE 0 END AS INTEGER)))   as {{ title }}_in_{{ time_name }}time_3600_plus
        {% endfor %}
        {% endfor %}

      From {{ dbc.log }} as dbql
      /* TODO: union with DBQL_Summary table - Paul */

      join dim_user usr
        on dbql.UserName = usr.UserName

      where {{ dbc.logdate('dbql') }} {{ date_range() }}

      Group by LogDate
        , usr.User_Bucket
        , usr.User_Department
        , usr.User_SubDepartment

{% for file, suffix in [('cpu_summary', '_maxcpu'), ('DBQL_Core', ''), ('Query_Breakouts', '_QryCnt_Ranges')] %}

- name: load {{ file }} to stage
  connect: transcend
  import:
    file: {{ file }}.csv
    table: {{ db_stg | default('adlste_coa_stg') }}.stg_dat_dbql_core{{ suffix }}

- name: merge {{ file }} from stage to core
  connect: transcend
  call:
    proc: {{ db_coa | default('adlste_coa') }}.sp_dat_dbql_core{{ suffix }}
    params:
      - *version
{% endfor %}
